{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rSb6dazd2AaL"
      ],
      "authorship_tag": "ABX9TyOvVf7ZSsaUo/gcHSsTC/a1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadarsh73/Python/blob/main/NLP/stemming_lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7EOHgbxyixp",
        "outputId": "f1745ca3-7b0d-410a-da59-7074b19397b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEMMING\n"
      ],
      "metadata": {
        "id": "1WSAVJPczFWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "ww-FSuowyoC3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
        "\n",
        "for word in words:\n",
        "  print(word, \"|\", stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9te4XCzytFr",
        "outputId": "378575d4-5e82-42a0-ff2b-95a329807ba9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating | eat\n",
            "eats | eat\n",
            "eat | eat\n",
            "ate | ate\n",
            "adjustable | adjust\n",
            "rafting | raft\n",
            "ability | abil\n",
            "meeting | meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LEMMATIZATION"
      ],
      "metadata": {
        "id": "hG1juam7zK0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"eating eats eat ate adjustable rafting ability ability meeting better ordering to\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token, \"|\", token.lemma_, \"|\", token.lemma)  #token.lemma give the hash of the base word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAiV33O6zOn8",
        "outputId": "42eff207-1106-4c78-fe1b-76496ac79ebd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating | eating | 12092082220177030354\n",
            "eats | eat | 9837207709914848172\n",
            "eat | eat | 9837207709914848172\n",
            "ate | eat | 9837207709914848172\n",
            "adjustable | adjustable | 6033511944150694480\n",
            "rafting | rafting | 1196139325854331\n",
            "ability | ability | 11565809527369121409\n",
            "ability | ability | 11565809527369121409\n",
            "meeting | meeting | 14798207169164081740\n",
            "better | well | 4525988469032889948\n",
            "ordering | order | 13136985495629980461\n",
            "to | to | 3791531372978436496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-cgcSymzlEC",
        "outputId": "07cb5a00-c757-4901-86ef-3ca5b56a84e1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ar = nlp.get_pipe('attribute_ruler')\n",
        "\n",
        "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]],{\"LEMMA\":\"Brother\"})  # Modify the lemma for some words\n",
        "\n",
        "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text, \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FooPruHm0JEo",
        "outputId": "6b9ebc0f-90df-4226-f27a-9b0e6767c8a0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bro | Brother\n",
            ", | ,\n",
            "you | you\n",
            "wanna | wanna\n",
            "go | go\n",
            "? | ?\n",
            "Brah | Brother\n",
            ", | ,\n",
            "do | do\n",
            "n't | not\n",
            "say | say\n",
            "no | no\n",
            "! | !\n",
            "I | I\n",
            "am | be\n",
            "exhausted | exhaust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nuvVWzri0X3q"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing stop words"
      ],
      "metadata": {
        "id": "rSb6dazd2AaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j0-jvTZ2D69",
        "outputId": "96d8a870-c837-462f-d8ec-c1c934d917fe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "  Wikipedia[note 3] is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration Wikipedia was launched by Jimmy Wales and L\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# print(stop_words)"
      ],
      "metadata": {
        "id": "1zUPwppM2Ops"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlASOsnHRG3W",
        "outputId": "22ab3a69-ce43-413a-92cc-c61f01b5efbd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUXPhmdET4fl",
        "outputId": "9ba1d799-0ab4-4716-93d4-b905e34cb52b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "tokenized_words = word_tokenize(text)\n",
        "\n",
        "without_stop_words = []\n",
        "for word in tokenized_words:\n",
        "  if word not in stop_words:\n",
        "    without_stop_words.append(word)\n",
        "\n",
        "print(without_stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9lkCwfO2cdY",
        "outputId": "134e87ad-32f4-4737-81dc-ca32b95fa4dc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Wikipedia', '[', 'note', '3', ']', 'multilingual', 'free', 'online', 'encyclopedia', 'written', 'maintained', 'community', 'volunteers', ',', 'known', 'Wikipedians', ',', 'open', 'collaboration', 'Wikipedia', 'launched', 'Jimmy', 'Wales', 'L']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uc6iZBQI8I3w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}